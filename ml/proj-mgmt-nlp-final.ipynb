{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4c712d",
   "metadata": {},
   "source": [
    "# NLP for Project Management\n",
    "Model development to tag project management status reports with risk level tags \"Red\", \"Amber\", and \"Green\".\n",
    "\n",
    "A sample dataset is used with the fastai (v2) library to demonstrate steps involved in model development. fastai uses a transfer learning approach which is relatively recent (2019) in NLP practice. Originating in image recognition modeling, transfer learning leverages pre-trained (usually large) models and adds or adjusts the final layers for specific classification. Classic NLP, before the advent of deep neural networks and transfer learning involved tokenization and statistical analysis of word occurences. Before jumping into the transfer learning demonstration this notebook presents some classic statistical analysis. In addition to background on the history of NLP, the statistical analysis gives sanity checks on the dataset and adheres to the very modern principle that the more you check and recheck your data, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2ea56",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Run this notebook in the conda environment defined in the environment yaml file for the project.\n",
    "\n",
    "Import the fastai.text modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55527a8",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The sample dataset for this notebook is in a CSV file 'pmd-nlp.csv'. First, simply read the data into a pandas dataframe and check to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a83bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pmd-nlp.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969c210",
   "metadata": {},
   "source": [
    "Notice the three columns **'label'**, **'text'**, and **'is_valid'**. This is a manually labeled dataset. The text of the status reports appears in the 'text' column, the Red-Amber-Green labels appear in the 'label' column, and the 'is_valid' column indicates the training, validation split. Reports marked for validation will have 'True' in the 'is_valid' column. Note that fastai dataloaders can manage training, validation split in a number of ways which may override the use of the is_valid setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd98bc",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing\n",
    "After loading the panda dataframe, the next step is to scan character strings and parse the text into word tokens. The tokens will then be tagged numerically in order to simplify working with integers instead of character strings. The integer tags will also serve as column indices of a matrix of occurence counts called the *document term* matrix. The numericalization step will also build the corpus *vocabulary* which is a ranked list of words. The rank is by frequency of occurence in the total dataset and cutoff at a minimum  frequency value, typically 3 or 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1772e1",
   "metadata": {},
   "source": [
    "#### The Dataframe\n",
    "Notice the fundamental reference mechanism to the elements of the dataframe, **df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3225474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa074612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35055f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.is_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86035c79",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "The default word tokenizer in fastai is spaCy. It is easy to find specific information on spaCy. A couple starting points are, https://spacy.io/usage/spacy-101 and https://machinelearningknowledge.ai/complete-guide-to-spacy-tokenizer-with-examples/\n",
    "\n",
    "Here is an example of the default tokenization of the text from the first status report in the dataset. WordTokenizer is a generator hence we need to use first() to get the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52177930",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = WordTokenizer()\n",
    "tokens = first(spacy([df.text[0]]))\n",
    "print(coll_repr(tokens, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85aac16",
   "metadata": {},
   "source": [
    "Define the tokenize function based on the spaCy generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = Tokenizer(spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b49ff5",
   "metadata": {},
   "source": [
    "Tokenize all (708 count) reports in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112cb1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_corpus = df.text.map(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c1828",
   "metadata": {},
   "source": [
    "Check the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59851f16",
   "metadata": {},
   "source": [
    "Look at the last report (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_corpus[707]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7afbb",
   "metadata": {},
   "source": [
    "#### Split training and validation\n",
    "Split train and valid based on is_valid column. Also store the label in a tuple with the token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fe778",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttxt_train = [(tokens_corpus[i], df.label[i]) for i in range(len(tokens_corpus)) if not df.is_valid[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab8110",
   "metadata": {},
   "source": [
    "Record the count of training reports (expect 606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38487b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_count_training = len(ttxt_train); report_count_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttxt_valid = [(tokens_corpus[i], df.label[i]) for i in range(len(tokens_corpus)) if df.is_valid[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98004f",
   "metadata": {},
   "source": [
    "Record the count of validation reports (expect 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50af127",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_count_validation = len(ttxt_valid); report_count_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f6925",
   "metadata": {},
   "source": [
    "The first report in the training set, with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f06245",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttxt_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e495d6",
   "metadata": {},
   "source": [
    "and the last report in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttxt_train[605]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb3d2f",
   "metadata": {},
   "source": [
    "which is also the last report in the corpus, remember that the training, validation and corpus indices are not in synch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[707]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624af46a",
   "metadata": {},
   "source": [
    "but the df and corpus indices are in sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5083bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_corpus[707]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b95ca7",
   "metadata": {},
   "source": [
    "#### Numericalization\n",
    "The word tokens are mapped to integers ranked in decreasing order of frequency of occurence. The default minimum frequency is 3. Since the dataset is on the small side, we'll use a minimum frequency value of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc166b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerize = Numericalize(min_freq=2)\n",
    "numerize.setup(tokens_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083b440",
   "metadata": {},
   "source": [
    "Check the length of the vocabulary. Note how the vocabulary list is referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_count = len(numerize.vocab); term_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22138887",
   "metadata": {},
   "source": [
    "Set a friendlier handle and check the last 3 words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb809735",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab = numerize.vocab\n",
    "full_vocab[1661:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba66ad7",
   "metadata": {},
   "source": [
    "is 'funding' in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142bfb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'funding' in full_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab.index('funding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0adba72",
   "metadata": {},
   "source": [
    "Look both ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab[908]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae75fe",
   "metadata": {},
   "source": [
    "List indices of all appearances of x and verification that words are listed only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i,x in enumerate(full_vocab) if x=='funding']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77398769",
   "metadata": {},
   "source": [
    "Can also check all the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(full_vocab.count(w),w) for w in full_vocab if full_vocab.count(w)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e984fd",
   "metadata": {},
   "source": [
    "### Assemble the doc (report) term matrix individually for the training and validation reports\n",
    "report_count_training|validation = number of reports in the training|validation set\n",
    "term_count = number of words (terms) in vocabulary\n",
    "\n",
    "The doc_term matrices will be report_count_training|validation rows by term_count columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93577ae9",
   "metadata": {},
   "source": [
    "#### Embedding vector\n",
    "The document, term matrix is just a stack of embedding vectors for for the 606 | 102 documents in the training|validation document set. The number of elements in an embedding vector is equal to the length of the vocabulary list (term_count=1664). Each element of the vector records the occurence count for the word at that index in the document the vector represents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c6a91",
   "metadata": {},
   "source": [
    "Make the embedding vector for the first document as an example.\\\n",
    "Start with the token text for ttxt_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttxt_train[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e231df",
   "metadata": {},
   "source": [
    "next, numericalize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7786e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc_nums = numerize(ttxt_train[0][0]); first_doc_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64635f8b",
   "metadata": {},
   "source": [
    "Then count the number of times each value occurs, and store the count at the word index tied to that value.\n",
    "\n",
    "For example, 2 occurs once and corresponds to xxbos so store a 1 at index 2.\n",
    "\n",
    "Note: this is not an efficient method for counting occurences, for a small dataset and vocabulary it is okay to be inefficient, change it if you have a big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([list(first_doc_nums).count(i) for i in range(len(full_vocab))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fdd808",
   "metadata": {},
   "source": [
    "Define a function to assemble (stack) embedding vectors into a doc term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_docterm_matrix(ndocs, nterms, ttxt):\n",
    "    # ndocs = report_count_training|validation\n",
    "    # nterms = term_count\n",
    "    # ttxt = tokenized text, one row per document\n",
    "    # the first row of the matrix is from report ttxt[0]\n",
    "    embv = numerize(ttxt[0][0])\n",
    "    doc_term_matrix = np.array([list(embv).count(i) for i in range(nterms)])\n",
    "    \n",
    "    # append the remaining ndocs-1 embedding vectors to the matrix\n",
    "    for di in range(1, ndocs):\n",
    "        embv = numerize(ttxt[di][0])\n",
    "        doc_term_matrix = np.vstack((doc_term_matrix,np.array([list(embv).count(i) for i in range(nterms)])))\n",
    "\n",
    "    return doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7985c",
   "metadata": {},
   "source": [
    "Create the matrix, expect this to take 15 to 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d933ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_train = assemble_docterm_matrix(report_count_training, term_count, ttxt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b23b8",
   "metadata": {},
   "source": [
    "Saving the matrix to a file will save recreating it after the notebook kernel restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('doc_term_train.npy', doc_term_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102eec59",
   "metadata": {},
   "source": [
    "Use this to load the matrix from file if you have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5843fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_train = np.load('doc_term_train.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345b45d",
   "metadata": {},
   "source": [
    "#### Sparse Matrix representation\n",
    "The doc term matrix is very sparse, 98% as seen below. Normally a sparse representation like CSR would be used but with a small dataset we won't bother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_non0 = np.count_nonzero(doc_term_train)\n",
    "sparsity = (doc_term_train.size - dtm_non0)/doc_term_train.size\n",
    "print(f'All the {doc_term_train.size} elements of the doc term matrix are zero except for {dtm_non0}')\n",
    "print(f'that makes sparsity at {sparsity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.spy(doc_term_train, markersize=0.10, aspect = 'auto')\n",
    "fig.set_size_inches(8,6)\n",
    "fig.savefig('doc_term_train_matrix.png', dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004910d",
   "metadata": {},
   "source": [
    "Assemble the validation doc term matrix in the same manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092aa60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_valid = assemble_docterm_matrix(report_count_validation, term_count, ttxt_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_valid[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c8f4b",
   "metadata": {},
   "source": [
    "Optionally save the matrix, this one is 1/6th the size of the training version so can easily be regenerated dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('doc_term_valid.npy', doc_term_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330e648",
   "metadata": {},
   "source": [
    "Use this to load the matrix from file if you have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98859a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_valid = np.load('doc_term_valid.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211218a",
   "metadata": {},
   "source": [
    "Also very sparse, 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59911615",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_non0 = np.count_nonzero(doc_term_valid)\n",
    "sparsity = (doc_term_valid.size - dtm_non0)/doc_term_valid.size\n",
    "print(f'All the {doc_term_valid.size} elements of the doc term matrix are zero except for {dtm_non0}')\n",
    "print(f'that makes sparsity at {sparsity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6de8cb",
   "metadata": {},
   "source": [
    "### Data review summary\n",
    "The data has been split into training and validation sets based on the column 'is_valid'.\n",
    "\n",
    "#### Data items\n",
    "**df** has 708 rows\\\n",
    "  df.label\\\n",
    "  df.text\\\n",
    "  df.is_valid\n",
    "\n",
    "**tokens_corpus** is df.text tokenized by tkn = Tokenizer(spacy)\n",
    "\n",
    "**ttxt_train** is training subset of toks_all in tuple form with labels (toks_all[i], df.label[i])\\\n",
    "  has report_count_training = 606 rows\n",
    "\n",
    "**ttxt_valid** is validation subset of toks_all in tuple form with labels\\\n",
    "  has report_count_validation = 102 rows\n",
    "\n",
    "**full_vocab** is dataset vocabulary from numericalization with minimum frequency set to 2. The vocabulary is ordered in decending frequency of occurence in the dataset.\\\n",
    "**term_count** is 1664, the size of the vocabulary\n",
    "\n",
    "**doc_term_train** is the 606 x 1664 matrix of stacked embedding vectors for training reports giving the occurence count of each word in the vocabulary per report\n",
    "\n",
    "**doc_term_valid** is the 102 x 1664 matrix of stacked embedding vectors for validation reports giving the occurence count of each word in the vocabulary per report\n",
    "\n",
    "The label counts from the original csv dataset are:\\\n",
    "*GREEN* training = 275, validation = 38\\\n",
    "*AMBER* training = 195, validation = 31\\\n",
    "*RED* training = 136, validation = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b927b",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "Things are collected and prepared to do some analysis. Basic analysis like proportion of reports with each label, *RED*, *AMBER*, and *GREEN*. Most frequently occurring words in Red reports etc. Using the analysis, we'll see a way to make a classifier based on it called Naive Bayes.\n",
    "\n",
    "List the indices for GREEN, AMBER, and RED reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206dbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_t_indices = [i for i in range(len(ttxt_train)) if ttxt_train[i][1]=='GREEN']\n",
    "green_v_indices = [i for i in range(len(ttxt_valid)) if ttxt_valid[i][1]=='GREEN']\n",
    "amber_t_indices = [i for i in range(len(ttxt_train)) if ttxt_train[i][1]=='AMBER']\n",
    "amber_v_indices = [i for i in range(len(ttxt_valid)) if ttxt_valid[i][1]=='AMBER']\n",
    "red_t_indices = [i for i in range(len(ttxt_train)) if ttxt_train[i][1]=='RED']\n",
    "red_v_indices = [i for i in range(len(ttxt_valid)) if ttxt_valid[i][1]=='RED']\n",
    "print(f'GREEN training = {len(green_t_indices)}, validation = {len(green_v_indices)}\\nAMBER training = {len(amber_t_indices)}, validation = {len(amber_v_indices)}\\nRED training = {len(red_t_indices)}, validation = {len(red_v_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f34f8",
   "metadata": {},
   "source": [
    "The label counts match the given values in the input CSV file. The first data consistency check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630e027",
   "metadata": {},
   "source": [
    "#### class priors\n",
    "The proportion of report count for each label value to the whole set.\\\n",
    "gpt = ratio of green reports to total in training\\\n",
    "etc.\\\n",
    "\n",
    "Should have:\\\n",
    "gpt + apt + rpt = 1\\\n",
    "gpv + apv + rpv = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnt = len(green_t_indices)\n",
    "gnv = len(green_v_indices)\n",
    "ant = len(amber_t_indices)\n",
    "anv = len(amber_v_indices)\n",
    "rnt = len(red_t_indices)\n",
    "rnv = len(red_v_indices)\n",
    "gpt = gnt/report_count_training\n",
    "gpv = gnv/report_count_validation\n",
    "apt = ant/report_count_training\n",
    "apv = anv/report_count_validation\n",
    "rpt = rnt/report_count_training\n",
    "rpv = rnv/report_count_validation\n",
    "print(f'GREEN training percentage = {gpt}\\nGREEN validation percentage = {gpv}\\nAMBER training percentage = {apt}\\nAMBER validation percentage = {apv}\\nRED training percentage = {rpt}\\nRED training percentage = {rpv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt+apt+rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258d6f2",
   "metadata": {},
   "source": [
    "#### Occurence counts\n",
    "The occurence count vectors per label value are the sum of the embedding vectors with that label.\n",
    "\n",
    "Green"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30d96f",
   "metadata": {},
   "source": [
    "Occurence Count vector for GREEN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OC_green = np.zeros(term_count)\n",
    "for pos in range(len(green_t_indices)):\n",
    "    OC_green += doc_term_train[green_t_indices[pos]]\n",
    "OC_green[:44]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e01be60",
   "metadata": {},
   "source": [
    "Notice the 275 count at index 2. That index corresponds to 'xxbos', the 'beginning of stream' token that occurs once at the start of each report. Since there are 275 *GREEN* reports in the training dataset, this is a consistency check.\n",
    "\n",
    "Amber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377de845",
   "metadata": {},
   "source": [
    "Occurence Count vector for AMBER training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "OC_amber = np.zeros(term_count)\n",
    "for pos in range(len(amber_t_indices)):\n",
    "    OC_amber += doc_term_train[amber_t_indices[pos]]\n",
    "OC_amber[:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce277e",
   "metadata": {},
   "source": [
    "The 195 value at index 2 looks good for 'xxbos'.\n",
    "\n",
    "Red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ad4ab",
   "metadata": {},
   "source": [
    "Occurence Count vector for RED training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b367bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OC_red = np.zeros(term_count)\n",
    "for pos in range(len(red_t_indices)):\n",
    "    OC_red += doc_term_train[red_t_indices[pos]]\n",
    "OC_red[:44]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f9ab5",
   "metadata": {},
   "source": [
    "The 136 value at index 2 checks out.\n",
    "\n",
    "Let's trace some occurence counts to verify things look okay.\n",
    "\n",
    "The first *Red* report in training is: (refer to df.head() also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defefca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(ttxt_train[red_t_indices[0]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c087906",
   "metadata": {},
   "source": [
    "Notice the report has the word 'table' four times and the word 'tables' one time -- check if this is correct in the doc term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab.index('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_train[red_t_indices[0]][223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9117c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There is {doc_term_train[red_t_indices[0]][full_vocab.index(\"tables\")]} occurence of \"tables\" in the first Red report')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69636895",
   "metadata": {},
   "source": [
    "Check how often 'project' appears in GREEN, AMBER and RED reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_index = full_vocab.index('project')\n",
    "print(f'The word \"project\" appears {OC_red[term_index]} and {OC_amber[term_index]} and {OC_green[term_index]} times in red, amber and green documents, respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89a3af",
   "metadata": {},
   "source": [
    "Check how often 'completed' appears in GREEN, AMBER and RED reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_index = full_vocab.index('completed')\n",
    "print(f'The word \"completed\" appears {OC_red[term_index]} and {OC_amber[term_index]} and {OC_green[term_index]} times in red, amber and green documents, respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9144613",
   "metadata": {},
   "source": [
    "GREEN reports with the word project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_index = full_vocab.index('project')\n",
    "gr_prj_indices = [i for i in range(len(ttxt_train)) if (ttxt_train[i][1]=='GREEN') and (doc_term_train[i,term_index]>0)]\n",
    "gr_prj_indices[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336b081",
   "metadata": {},
   "source": [
    "Look at the one at index 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "' '.join(ttxt_train[600][0]), ttxt_train[600][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c889f",
   "metadata": {},
   "source": [
    "Look at the counts, there are supposed to be 3 occurences of \"project\" in report 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf867b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc_term_train[i,term_index] for i in gr_prj_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9ab66",
   "metadata": {},
   "source": [
    "#### Conditional likelihood\n",
    "L(t|green) likelihood term (word) appears in a GREEN report is OC_green/gnt\n",
    "\n",
    "Once we have conditional likelihood vectors, the commonly occurring Red, Amber, and Green words can be identified.\n",
    "\n",
    "Conditional likelihood vectors for each label value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e88dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_green = (OC_green + 1) / (gnt + 1)\n",
    "CL_amber = (OC_amber + 1) / (ant + 1)\n",
    "CL_red = (OC_red + 1) / (rnt + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ab707",
   "metadata": {},
   "source": [
    "Log ratios between each pair of label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ed5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rga = np.log(CL_green/CL_amber)\n",
    "Rgr = np.log(CL_green/CL_red)\n",
    "Rar = np.log(CL_amber/CL_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beb4f12",
   "metadata": {},
   "source": [
    "GREEN to RED comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de68ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_tokens = 10\n",
    "top_Rgr = np.argpartition(Rgr, -n_tokens)[-n_tokens:]\n",
    "bot_Rgr = np.argpartition(Rgr, n_tokens)[:n_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e414e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Top {n_tokens} log-count ratios: {Rgr[list(top_Rgr)]}\\n')\n",
    "print(f'Bottom {n_tokens} log-count ratios: {Rgr[list(bot_Rgr)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec7510",
   "metadata": {},
   "source": [
    "Green words in green to red comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[full_vocab[i] for i in top_Rgr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397733b0",
   "metadata": {},
   "source": [
    "Not necessarily all *Green* sounding words, but not unexpected either. 'well', 'soon', 'working', 'good', and 'track' sound *Green*\n",
    "\n",
    "Red words in green to red comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[full_vocab[i] for i in bot_Rgr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8be7e",
   "metadata": {},
   "source": [
    "'cancelled', 'without', and 'unable' look the most *Red* here\n",
    "\n",
    "AMBER to RED comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f24f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 10\n",
    "top_Rar = np.argpartition(Rar, -n_tokens)[-n_tokens:]\n",
    "bot_Rar = np.argpartition(Rar, n_tokens)[:n_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb21f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Top {n_tokens} log-count ratios: {Rar[list(top_Rar)]}\\n')\n",
    "print(f'Bottom {n_tokens} log-count ratios: {Rar[list(bot_Rar)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3653b5",
   "metadata": {},
   "source": [
    "Amber words in amber to red comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "[full_vocab[i] for i in top_Rar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4cd1c",
   "metadata": {},
   "source": [
    "Red words in amber to red comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "[full_vocab[i] for i in bot_Rar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f93183",
   "metadata": {},
   "source": [
    "'without' and maybe 'milestone', as in missed milestone are the only *Red* sounding words ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a64c9",
   "metadata": {},
   "source": [
    "GREEN to AMBER comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b459ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_tokens = 10\n",
    "top_Rga = np.argpartition(Rga, -n_tokens)[-n_tokens:]\n",
    "bot_Rga = np.argpartition(Rga, n_tokens)[:n_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99901341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Top {n_tokens} log-count ratios: {Rga[list(top_Rga)]}\\n')\n",
    "print(f'Bottom {n_tokens} log-count ratios: {Rga[list(bot_Rga)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c61f8",
   "metadata": {},
   "source": [
    "Green words in green to amber comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b38139",
   "metadata": {},
   "outputs": [],
   "source": [
    "[full_vocab[i] for i in top_Rga]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc09da0",
   "metadata": {},
   "source": [
    "Amber words in green to amber comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[full_vocab[i] for i in bot_Rga]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da94aea",
   "metadata": {},
   "source": [
    "'impacted', 'delay', 'divestiture', and 'risk' sound *Amber*\n",
    "\n",
    "RED reports with the word risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_index = full_vocab.index('risk')\n",
    "red_risk_indices = [i for i in range(len(ttxt_train)) if (ttxt_train[i][1]=='RED') and (doc_term_train[i,term_index]>0)]\n",
    "red_risk_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89b66c",
   "metadata": {},
   "source": [
    "RED reports with the word risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d18089",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_index = full_vocab.index('risks')\n",
    "red_risks_indices = [i for i in range(len(ttxt_train)) if (ttxt_train[i][1]=='RED') and (doc_term_train[i,term_index]>0)]\n",
    "red_risks_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf411387",
   "metadata": {},
   "source": [
    "Dataset bias, log ratio of green to amber, red to amber and red to green labeled items in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e7d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_green_red = np.log(gpt/rpt)\n",
    "b_amber_red = np.log(apt/rpt)\n",
    "b_green_amber = np.log(gpt/apt)\n",
    "print(f'bias values for green to red = {b_green_red}\\n                amber to red = {b_amber_red}\\n              green to amber = {b_green_amber}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b4cbcb",
   "metadata": {},
   "source": [
    "### A Naive Bayes Classifier\n",
    "Use the doc_term_valid validation report matrix binarized so weights are 1 or 0. 1 if the word occurs one or more times and 0 if it is absent. Add the appropriate bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f46ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.sign(doc_term_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a89f7",
   "metadata": {},
   "source": [
    "Predict labels for the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1142aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_amber_red = ['AMBER' if p else 'RED' for p in (W @ Rar + b_amber_red) > 0]\n",
    "preds_green_red = ['GREEN' if p else 'RED' for p in (W @ Rgr + b_green_red) > 0]\n",
    "preds_green_amber = ['GREEN' if p else 'AMBER' for p in (W @ Rga + b_green_amber) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17483f2",
   "metadata": {},
   "source": [
    "Two out of three seems like a reasonable prediction\n",
    "\n",
    "Compare these to the validation labels to get an accuracy for the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_preds = [[preds_amber_red[i], preds_green_red[i], preds_green_amber[i]] for i in range(len(preds_amber_red))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab9e47",
   "metadata": {},
   "source": [
    "### Analysis Summary\n",
    "The statistical analysis gives a feel for the quality of the data and the quality of the labelling. The Naive Bayes classifier is historically interesting. Distinction between *Green* and *Amber* words in the dataset may be problematic based on the statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0ecc4",
   "metadata": {},
   "source": [
    "## Neural Network NLP\n",
    "The approach here, using fastai, is to develop a language model based on the vocabulary of the dataset.\n",
    "\n",
    "dls_lm is the language model and created from the dataframe **df** as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_lm = TextDataLoaders.from_df(df,\n",
    "                                 text_col = 'text',\n",
    "                                 label_col = 'label',\n",
    "                                 valid_pct = 0.20,\n",
    "                                 bs = 64,\n",
    "                                 is_lm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ec430",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815cc67",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "Next, the language model is integrated with the core model, AWD_LSTM. AWD_LSTM define the model architecture which is described in https://arxiv.org/abs/1708.02182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9f151",
   "metadata": {},
   "source": [
    "unfreeze the layers of the model to update weights according to our specific dataset vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c128216",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68527f",
   "metadata": {},
   "source": [
    "save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86678995",
   "metadata": {},
   "source": [
    "The retrained model can now be used as a predictor to predict next words. Here, given the initial text, 40 more words are generated, twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afbd73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"The project has some risks\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
    "         for _ in range(N_SENTENCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff41618",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e961d4",
   "metadata": {},
   "source": [
    "Now create a classifier. The validation split is set at 20% which will ignore the 'is_valid' column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064aab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = TextDataLoaders.from_df(df,\n",
    "                        valid_pct = 0.2,\n",
    "                        text_col = 'text',\n",
    "                        label_col = 'label',\n",
    "                        bs = 64,\n",
    "                        text_vocab = dls_lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74273a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d59c0d",
   "metadata": {},
   "source": [
    "Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,\n",
    "                                metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b83bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4dacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57b09b",
   "metadata": {},
   "source": [
    "We can use the following to help select a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4611df",
   "metadata": {},
   "source": [
    "A confusion matrix helps point out ambiguities. Here we see similar *Green/Amber* fuzziness we saw in the statistical analysis of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ad7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bc73c",
   "metadata": {},
   "source": [
    "### Sample classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fe577",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict('There is a high risk of staffing loss.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad047d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict('No work was completed and there is no support.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict('Everything is on schedule and support is standing by if needed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecaa4ec",
   "metadata": {},
   "source": [
    "Save the model in export.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe456a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2280af0",
   "metadata": {},
   "source": [
    "#### Running on CPU or GPU\n",
    "use the code below to check the cuda device status for the execution environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.cuda.get_device_name(0) if torch.cuda.is_available() else None\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e7b46",
   "metadata": {},
   "source": [
    "Copyright (c) 2022, Oracle and/or its affiliates.\n",
    "\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "043da1a5ba4cc66f7e60d31a996a4e2039da24d8a03a670d7ea880b4a1ad34f7"
  },
  "kernelspec": {
   "display_name": "Python [conda env:fastai2-gpu]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
